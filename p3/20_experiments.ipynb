{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import os \n", "import pandas as pd\n", "import numpy as np\n", "from sklearn.metrics import accuracy_score, roc_curve, confusion_matrix, classification_report, roc_auc_score, auc\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, cross_validate, StratifiedKFold\n", "from sklearn.model_selection import GridSearchCV\n", "import sys \n", "from sklearn.model_selection import learning_curve\n", "from sklearn.ensemble import RandomForestClassifier\n", "from sklearn.datasets import make_classification\n", "from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.naive_bayes import GaussianNB, MultinomialNB, ComplementNB, CategoricalNB\n", "from sklearn.neighbors import KNeighborsClassifier\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.metrics import precision_recall_curve, average_precision_score, make_scorer, precision_score, recall_score, f1_score\n", "from scipy import stats\n", "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, RFECV\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from statistics import mean "]}, {"cell_type": "markdown", "metadata": {}, "source": ["############## UTILITY FUNCTIONS ###################<br>\n", "display test scores"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def t_test(results_array, model_names, score_name):\n", "    string=\"\"\n", "    for i in range(len(model_names) - 1):\n", "           for j in range(i, len(model_names)):\n", "               if i == j:\n", "                   continue\n", "               t, p = stats.ttest_ind(results_array[i], results_array[j], equal_var=False)\n", "               string += \"\\n\"+score_name\n", "               string += \"T_Test between {} & {}: T Value = {}, P Value = {}\".format(model_names[i], model_names[j], t, p)\n", "               if p>0.05:\n", "                   string += \"p-value>0.05 so there's NO significant difference between models.\"\n", "               else:\n", "                   string += \"p-value<=0.05 so there's A significant difference between models.\" \n", "    return string"]}, {"cell_type": "markdown", "metadata": {}, "source": ["plot ROC "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_roc(fpr,tpr,model_name,selector_name):\n", "    plt.figure()\n", "    lw = 2\n", "    plt.plot(fpr, tpr, color='darkorange',\n", "             lw=lw, label='ROC curve (area = %0.2f)' % auc(fpr,tpr))\n", "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n", "    plt.xlim([0.0, 1.0])\n", "    plt.ylim([0.0, 1.05])\n", "    plt.xlabel('False Positive Rate')\n", "    plt.ylabel('True Positive Rate')\n", "    plt.title(model_name + \"/\" + selector_name)\n", "    plt.legend(loc=\"lower right\")\n", "    \n", "################ NORMALIZATION ###################\n", "df_original = pd.read_pickle('final_step2.pkl')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = df_original.loc[:, [\"n_EAR\", \n", "                    \"n_MAR\", \"n_MOE\", \"n_EC\",\n", "                    \"n_LEB\", \"n_SOP\", \"PERCLOS\", \"CLOSENESS\"]]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y = df_original.loc[:, \"DROWSINESS\"]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["normalize each columns"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\n", "scaler.fit(X)\n", "X_scaled=scaler.transform(X)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############## MODEL and SCORE DEFINITIONS ###################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["models = []\n", "models.append(('KNN-5', KNeighborsClassifier(n_neighbors=5, n_jobs=-1)))\n", "models.append(('CART-gini', DecisionTreeClassifier(criterion=\"gini\"))) \n", "models.append(('NB', GaussianNB()))\n", "models.append(('KNN-25', KNeighborsClassifier(n_neighbors=25, n_jobs=-1)))\n", "models.append(('CART-entropy', DecisionTreeClassifier(criterion=\"entropy\"))) "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scoring = []\n", "scoring.append(('accuracy', accuracy_score))\n", "scoring.append(('prec', precision_score))\n", "scoring.append(('recall', recall_score))\n", "scoring.append(('f1', f1_score))\n", "scoring.append(('auc', roc_auc_score))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############## VARIABLES ###################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fold_info_list = []\n", "table1_output = \"\"\n", "fold_no = 0"]}, {"cell_type": "markdown", "metadata": {}, "source": ["############## OUTER CV FOR T-TEST (5 FOLD) ###################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["outer_cv = StratifiedKFold(n_splits=5,random_state=42, shuffle=True)\n", "for train_index, test_index in outer_cv.split(X_scaled, y): \n", "    X_train, X_test, y_train, y_test = X_scaled[train_index], X_scaled[test_index], y[train_index], y[test_index]\n", "    \n", "    ############# FEATURE SELECTION (4 METHODS: FULL, ANOVA, MI, RFE-RF) #############\n", "    table1_output += \"OUTER CV FOLD NO: {}\\n\".format(fold_no)\n", "    X_train_FULL = X_train\n", "    \n", "    ANOVA_selector = SelectKBest(f_classif, k=5)\n", "    X_train_ANOVA = ANOVA_selector.fit_transform(X_train, y_train)\n", "    print(\"ANOVA scores: {}, ANOVA p-values: {}\".format(ANOVA_selector.scores_, ANOVA_selector.pvalues_))\n", "    table1_output += \"ANOVA scores: {}, ANOVA p-values: {}\".format(ANOVA_selector.scores_, ANOVA_selector.pvalues_)\n", "    \n", "    MI_selector = SelectKBest(mutual_info_classif, k=5)\n", "    X_train_MI = MI_selector.fit_transform(X_train, y_train)\n", "    print(\"MI scores: {}, MI p-values: {}\".format(MI_selector.scores_, MI_selector.pvalues_))\n", "    table1_output += \"MI scores: {}, MI p-values: {}\".format(MI_selector.scores_, MI_selector.pvalues_)\n", "    \n", "    #estimator for recursive feature elimination\n", "    # estimator = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', class_weight= None, max_features = None, random_state = 42,n_jobs=-1)\n", "    estimator = DecisionTreeClassifier(random_state = 42)\n", "    #inner cv for recursive feature elimination\n", "    inner_cv = StratifiedShuffleSplit(n_splits=2,test_size=0.2, random_state = 42)\n", "    RFE_selector = RFECV(estimator, step=1, cv=inner_cv, n_jobs=-1)\n", "    X_train_RFE = RFE_selector.fit_transform(X_train, y_train)\n", "    print(\"RFE rankings: {}, RFE grid-scores: {}\".format(RFE_selector.ranking_, RFE_selector.grid_scores_))\n", "    table1_output += \"RFE rankings: {}, RFE grid-scores: {}\".format(RFE_selector.ranking_, RFE_selector.grid_scores_)\n", "    \n", "    ############# TEST RESULTS FOR FEATURE SELECTION METHODS #############\n", "    fold_info = {\n", "        model_name:{\n", "            'FULL': {\n", "                score_name:0 for score_name,score in scoring\n", "                },\n", "            'ANOVA': {\n", "                score_name:0 for score_name,score in scoring\n", "                },\n", "            'MI': {\n", "                score_name:0 for score_name,score in scoring\n", "                },\n", "            'RFE': {\n", "                score_name:0 for score_name,score in scoring\n", "                }\n", "            } for model_name, model in models\n", "        }\n", "    for model_name, model in models:\n", "        # transform X_test according to feature selection methods\n", "        X_test_FULL = X_test\n", "        X_test_ANOVA = ANOVA_selector.transform(X_test)\n", "        X_test_MI = MI_selector.transform(X_test)\n", "        X_test_RFE = RFE_selector.transform(X_test)\n", "        # make predictions\n", "        model.fit(X_train_FULL,y_train)\n", "        y_pred_FULL = model.predict(X_test_FULL)\n", "        model.fit(X_train_ANOVA,y_train)\n", "        y_pred_ANOVA = model.predict(X_test_ANOVA)\n", "        model.fit(X_train_MI,y_train)\n", "        y_pred_MI = model.predict(X_test_MI)\n", "        model.fit(X_train_RFE,y_train)\n", "        y_pred_RFE = model.predict(X_test_RFE)\n", "        # save evaluation metrics\n", "        for score_name, score in scoring:\n", "            score_FULL = score(y_test,y_pred_FULL)\n", "            score_ANOVA = score(y_test,y_pred_ANOVA)\n", "            score_MI = score(y_test,y_pred_MI)\n", "            score_RFE = score(y_test,y_pred_RFE)\n", "            fold_info[model_name]['FULL'][score_name] = score_FULL,\n", "            fold_info[model_name]['ANOVA'][score_name] = score_ANOVA,\n", "            fold_info[model_name]['MI'][score_name] = score_MI,\n", "            fold_info[model_name]['RFE'][score_name] = score_RFE,\n\n", "        # save ROC metrics\n", "        fpr_FULL, tpr_FULL, _ = roc_curve(y_test,y_pred_FULL)\n", "        fold_info[model_name]['FULL']['ROC_fpr'] = fpr_FULL\n", "        fold_info[model_name]['FULL']['ROC_tpr'] = tpr_FULL\n", "        fpr_ANOVA, tpr_ANOVA, _ = roc_curve(y_test,y_pred_ANOVA)\n", "        fold_info[model_name]['ANOVA']['ROC_fpr'] = fpr_ANOVA\n", "        fold_info[model_name]['ANOVA']['ROC_tpr'] = tpr_ANOVA\n", "        fpr_MI, tpr_MI, _ = roc_curve(y_test,y_pred_MI)\n", "        fold_info[model_name]['MI']['ROC_fpr'] = fpr_MI\n", "        fold_info[model_name]['MI']['ROC_tpr'] = tpr_MI\n", "        fpr_RFE, tpr_RFE, _ = roc_curve(y_test,y_pred_RFE)\n", "        fold_info[model_name]['RFE']['ROC_fpr'] = fpr_RFE\n", "        fold_info[model_name]['RFE']['ROC_tpr'] = tpr_RFE\n", "        \n", "        # save confusion matrix\n", "        conf_FULL = confusion_matrix(y_test,y_pred_FULL)\n", "        fold_info[model_name]['FULL']['confusion_matrix'] = conf_FULL\n", "        conf_ANOVA = confusion_matrix(y_test,y_pred_ANOVA)\n", "        fold_info[model_name]['ANOVA']['confusion_matrix'] = conf_ANOVA\n", "        conf_MI = confusion_matrix(y_test,y_pred_MI)\n", "        fold_info[model_name]['MI']['confusion_matrix'] = conf_MI\n", "        conf_RFE = confusion_matrix(y_test,y_pred_RFE)\n", "        fold_info[model_name]['RFE']['confusion_matrix'] = conf_RFE\n", "            \n", "            \n", "    fold_info_list.append(fold_info)\n", "    fold_no += 1 "]}, {"cell_type": "markdown", "metadata": {}, "source": ["########### T-TEST #############"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results = {\n", "    \"FULL\": {score_name:[] for score_name,score in scoring},\n", "    \"ANOVA\": {score_name:[] for score_name,score in scoring},\n", "    \"MI\": {score_name:[] for score_name,score in scoring},\n", "    \"RFE\": {score_name:[] for score_name,score in scoring},\n", "    }"]}, {"cell_type": "markdown", "metadata": {}, "source": ["build result arrays for corresponding metrics"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for score_name, score in scoring:\n", "    for model_name, model in models:\n", "        model_results_FULL = []\n", "        model_results_ANOVA = []\n", "        model_results_MI = []\n", "        model_results_RFE = []\n", "        for fold_info in fold_info_list:\n", "             model_results_FULL.append(fold_info[model_name]['FULL'][score_name])\n", "             model_results_ANOVA.append(fold_info[model_name]['ANOVA'][score_name])\n", "             model_results_MI.append(fold_info[model_name]['MI'][score_name])\n", "             model_results_RFE.append(fold_info[model_name]['RFE'][score_name])\n", "        results['FULL'][score_name].append(model_results_FULL)\n", "        results['ANOVA'][score_name].append(model_results_ANOVA)\n", "        results['MI'][score_name].append(model_results_MI)\n", "        results['RFE'][score_name].append(model_results_RFE)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["calculate t-test on 5 folds for corresponding model and metric"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["t_test_output=\"\"\n", "model_names = [model_name for model_name, model in models]\n", "for score_name, score in scoring:\n", "        t_test_output += '\\nFULL'\n", "        results_array = results['FULL'][score_name]\n", "        t_test_output += t_test(results_array, model_names, score_name)\n", "        t_test_output += '\\nANOVA'\n", "        results_array = results['ANOVA'][score_name]\n", "        t_test_output += t_test(results_array, model_names, score_name)\n", "        t_test_output += '\\nMI'\n", "        results_array = results['MI'][score_name]\n", "        t_test_output += t_test(results_array, model_names, score_name)\n", "        t_test_output += '\\nRFE'\n", "        results_array = results['RFE'][score_name]\n", "        t_test_output += t_test(results_array, model_names, score_name)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["textfile = open('t-test.txt', 'w')\n", "textfile.write(t_test_output)\n", "textfile.close()   "]}, {"cell_type": "markdown", "metadata": {}, "source": ["########## EVALUATION METRICS #####################"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["metrics_output = \"\"\n", "for model_name, model in models:\n", "    metrics_output += \"\\n{} with full features\\n\".format(model_name)\n", "    for score_name, score in scoring:\n", "        metrics_output += \"{}: {},{},{},{},{}, average: {}\".format(score_name, *[fold_info[model_name][\"FULL\"][score_name][0] for fold_info in fold_info_list], mean([fold_info[model_name][\"FULL\"][score_name][0] for fold_info in fold_info_list]))\n", "    metrics_output += \"\\n{} with ANOVA\\n\".format(model_name)\n", "    for score_name, score in scoring:\n", "        metrics_output += \"{}: {},{},{},{},{}, average: {}\".format(score_name, *[fold_info[model_name][\"ANOVA\"][score_name][0] for fold_info in fold_info_list], mean([fold_info[model_name][\"ANOVA\"][score_name][0] for fold_info in fold_info_list]))\n", "    metrics_output += \"\\n{} with MI\\n\".format(model_name)\n", "    for score_name, score in scoring:\n", "        metrics_output += \"{}: {},{},{},{},{}, average: {}\".format(score_name, *[fold_info[model_name][\"MI\"][score_name][0] for fold_info in fold_info_list], mean([fold_info[model_name][\"MI\"][score_name][0] for fold_info in fold_info_list]))\n", "    metrics_output += \"\\n{} with RFE\\n\".format(model_name)\n", "    for score_name, score in scoring:\n", "        metrics_output += \"{}: {},{},{},{},{}, average: {}\".format(score_name, *[fold_info[model_name][\"RFE\"][score_name][0] for fold_info in fold_info_list], mean([fold_info[model_name][\"RFE\"][score_name][0] for fold_info in fold_info_list]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["textfile2 = open('metrics.txt', 'w')\n", "textfile2.write(metrics_output)\n", "textfile2.close()   "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["metrics_output2 = \"\"\n", "for model_name, model in models:\n", "    metrics_output += \"\\n{} with full features\\n\".format(model_name)\n", "    for score_name, score in scoring:\n", "        metrics_output += \"{}: {}\".format(score_name, mean([fold_info[model_name][\"FULL\"][score_name] for fold_info in fold_info_list]))\n", "    metrics_output += \"\\n{} with ANOVA\\n\".format(model_name)\n", "    for score_name, score in scoring:\n", "        metrics_output += \"{}: {}\".format(score_name, mean([fold_info[model_name][\"ANOVA\"][score_name][0] for fold_info in fold_info_list]))\n", "    metrics_output += \"\\n{} with MI\\n\".format(model_name)\n", "    for score_name, score in scoring:\n", "        metrics_output += \"{}: {}\".format(score_name, mean([fold_info[model_name][\"MI\"][score_name][0] for fold_info in fold_info_list]))\n", "    metrics_output += \"\\n{} with RFE\\n\".format(model_name)\n", "    for score_name, score in scoring:\n", "        metrics_output += \"{}: {}\".format(score_name, mean([fold_info[model_name][\"RFE\"][score_name][0] for fold_info in fold_info_list]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["textfile3 = open('metrics2.txt', 'w')\n", "textfile3.write(metrics_output2)\n", "textfile3.close()   \n", "        \n", "        \n", "############ ROC CURVES #########################\n", "# plot curves in grid\n", "plt.figure(figsize=(20,20))\n", "for i in range(0, 5):\n", "    plt.subplot(5, 4, 4*i+1)\n", "    fpr = np.mean([fold_info[model_names[i]]['FULL']['ROC_fpr'] for fold_info in fold_info_list], axis=0)\n", "    tpr = np.mean([fold_info[model_names[i]]['FULL']['ROC_tpr'] for fold_info in fold_info_list], axis=0)\n", "    plot_roc(fpr,tpr,model_names[i],\"FULL features\")\n", "    \n", "    plt.subplot(5, 4, 4*i+2)\n", "    fpr = np.mean([fold_info[model_names[i]]['ANOVA']['ROC_fpr'] for fold_info in fold_info_list], axis=0)\n", "    tpr = np.mean([fold_info[model_names[i]]['ANOVA']['ROC_tpr'] for fold_info in fold_info_list], axis=0)\n", "    plot_roc(fpr,tpr,model_names[i],\"ANOVA selector\")\n", "    \n", "    plt.subplot(5, 4, 4*i+3)\n", "    fpr = np.mean([fold_info[model_names[i]]['MI']['ROC_fpr'] for fold_info in fold_info_list], axis=0)\n", "    tpr = np.mean([fold_info[model_names[i]]['MI']['ROC_tpr'] for fold_info in fold_info_list], axis=0)\n", "    plot_roc(fpr,tpr,model_names[i],\"Mutual Info selector\")\n", "    \n", "    plt.subplot(5, 4, 4*i+4)\n", "    fpr = np.mean([fold_info[model_names[i]]['RFE']['ROC_fpr'] for fold_info in fold_info_list], axis=0)\n", "    tpr = np.mean([fold_info[model_names[i]]['RFE']['ROC_tpr'] for fold_info in fold_info_list], axis=0)\n", "    plot_roc(fpr,tpr,model_names[i],\"RFE selector with RF\")\n", "    \n", "    \n", "        \n", "############ CONFUSION MATRICES #########################    \n", "sns.set(font_scale=2.0) # for label size"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for no, fold_info in enumerate(fold_info_list,1):\n", "    plt.figure(figsize=(20,20))\n", "    plt.subplots_adjust(wspace = 0.5, hspace = 0.5)\n\n", "    # plt.title(\"FOLD NO: {}\".format(no))\n", "    for i in range(0, 5):\n", "        plt.subplot(5, 4, 4*i+1)\n", "        plt.title(model_names[i] + \"/\" + \"FULL\")\n", "        df_cm = pd.DataFrame(fold_info[model_names[i]]['FULL']['confusion_matrix'])\n", "        sns.heatmap(df_cm, annot=True, fmt=\"d\",cbar=False, annot_kws={\"size\": 18}) # font size\n", "        \n", "        plt.subplot(5, 4, 4*i+2)\n", "        plt.title(model_names[i] + \"/\" + \"ANOVA\")\n", "        df_cm = pd.DataFrame(fold_info[model_names[i]]['ANOVA']['confusion_matrix'])\n", "        sns.heatmap(df_cm, annot=True, fmt=\"d\",cbar=False, annot_kws={\"size\": 18}) # font size\n", "        \n", "        plt.subplot(5, 4, 4*i+3)\n", "        plt.title(model_names[i] + \"/\" + \"MI\")\n", "        df_cm = pd.DataFrame(fold_info[model_names[i]]['MI']['confusion_matrix'])\n", "        sns.heatmap(df_cm, annot=True, fmt=\"d\",cbar=False, annot_kws={\"size\": 18}) # font size\n", "        \n", "        plt.subplot(5, 4, 4*i+4)\n", "        plt.title(model_names[i] + \"/\" + \"RFE\")\n", "        df_cm = pd.DataFrame(fold_info[model_names[i]]['RFE']['confusion_matrix'])\n", "        sns.heatmap(df_cm, annot=True, fmt=\"d\", cbar=False, annot_kws={\"size\": 18}) # font size"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}